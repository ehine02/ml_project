# ml_project

Neural networks can be trained more efficiently through the application of optimisation techniques, specifically training algorithms. This report describes the evaluation of the WAME optimisation algorithm described in the paper ‘Training Convolutional Networks with Weight–wise Adaptive Learning Rates’  by Mosca and Magoulas (M&M) applied to a given dataset against evaluations of other established optimisation techniques, namely Adam, RMSProp, Adagrad and Stochastic Gradient Descent.

The objective is to quantitatively assess the WAME optimisation performance through the technical implementation of the algorithm with a selection of parameters. The premise outlined in the referenced paper is that WAME results in improved average loss rates and is generally more accurate, albeit negligibly so, compared to results using the other optimisation algorithms mentioned above.
